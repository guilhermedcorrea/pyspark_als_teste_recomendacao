{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_data(data: DataFrame) -> DataFrame:\n",
    "    indexer_customer_type = StringIndexer(inputCol=\"Customer type\", outputCol=\"Customer type Index\")\n",
    "    data = indexer_customer_type.fit(data).transform(data)\n",
    "\n",
    "    indexer_product_line = StringIndexer(inputCol=\"Product line\", outputCol=\"Product line Index\")\n",
    "    data = indexer_product_line.fit(data).transform(data)\n",
    "\n",
    "    numeric_columns = [\"Unit price\", \"Quantity\", \"Tax 5%\", \"Total\", \"Rating\"]\n",
    "    for col_name in numeric_columns:\n",
    "        data = data.withColumn(col_name, col(col_name).cast(FloatType()))\n",
    "\n",
    "    data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_and_preprocess_data(file_path: str) -> DataFrame:\n",
    "    spark = SparkSession.builder.appName(\"RestaurantRecommendationALS\").getOrCreate()\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "    preprocessed_data = preprocess_data(df)\n",
    "    return preprocessed_data\n",
    "\n",
    "def train_and_evaluate_model(data: DataFrame) -> ALS:\n",
    "    required_columns = [\"Customer type Index\", \"Product line Index\", \"Rating\"]\n",
    "    missing_columns = set(required_columns) - set(data.columns)\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Erro Valor nao encontrado: {missing_columns}\")\n",
    "\n",
    "    if data.count() == 0:\n",
    "        print(\"Erro Valor nao encontrado \")\n",
    "        return None\n",
    "\n",
    "    (train_data, test_data) = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "    als = ALS(maxIter=5, regParam=0.01, userCol=\"Customer type Index\", itemCol=\"Product line Index\", ratingCol=\"Rating\")\n",
    "    model = als.fit(train_data)\n",
    "\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"Rating\", predictionCol=\"prediction\")\n",
    "    mae = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "    return model\n",
    "\n",
    "def flatten_recommendations(recommendations: DataFrame) -> DataFrame:\n",
    "    return recommendations.select(\"Customer type Index\", explode(\"recommendations\").alias(\"recommendation\"))\n",
    "\n",
    "def get_recommendations_by_customer_id(spark: SparkSession, model: ALS, customer_id: Optional[int] = None, num_recommendations: int = 5) -> DataFrame:\n",
    "    if customer_id is not None:\n",
    "        customer_id_df = spark.createDataFrame([(customer_id,)], [\"Customer type Index\"])\n",
    "        user_recs = model.recommendForUserSubset(customer_id_df, num_recommendations)\n",
    "    else:\n",
    "        user_recs = model.recommendForAllUsers(num_recommendations)\n",
    "\n",
    "    user_recs_flattened = flatten_recommendations(user_recs)\n",
    "    return user_recs_flattened\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"supermarket_sales.csv\"\n",
    "    data = read_and_preprocess_data(file_path)\n",
    "    model = train_and_evaluate_model(data)\n",
    "\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"RestaurantRecommendationALS\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    user_id = 3\n",
    "    user_recs = get_recommendations_by_customer_id(spark, model, customer_id=user_id, num_recommendations=5)\n",
    "\n",
    "    user_recs_pandas = user_recs.toPandas()\n",
    "\n",
    "    print(f\"Top 5 Recommendations for Customer {user_id}:\")\n",
    "    print(user_recs_pandas)\n",
    "\n",
    "    all_user_recs = get_recommendations_by_customer_id(spark, model, num_recommendations=100)\n",
    "\n",
    "    all_user_recs_pandas = all_user_recs.toPandas()\n",
    "\n",
    "    print(\"Top 100 Recommendations for All Customers:\")\n",
    "    print(all_user_recs_pandas)\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Apache Livy Teste\n",
    "def run_spark_job_using_livy(livy_url: str, spark_code: str):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'code': spark_code,\n",
    "        'kind': 'spark',\n",
    "        'conf': {\n",
    "            'spark.jars.packages': 'org.apache.spark:spark-avro_2.12:3.1.1'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(f'{livy_url}/batches', json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 201:\n",
    "        batch_id = response.json()['id']\n",
    "        print(f\"Feito ! Batch ID: {batch_id}\")\n",
    "        return batch_id\n",
    "    else:\n",
    "        print(\"Erro\")\n",
    "        print(response.json())\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #exemplo de pyfile\n",
    "\n",
    "    livy_url = \"http:127.0.0.1:8998\"  \n",
    "    spark_code = \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"RecommendationALS\").getOrCreate()\n",
    "\n",
    "    # spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    # ... (teste sql server)\n",
    "\n",
    "    spark.stop()\n",
    "    \"\"\"\n",
    "    batch_id = run_spark_job_using_livy(livy_url, spark_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
